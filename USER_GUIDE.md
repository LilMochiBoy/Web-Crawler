# ğŸ•¸ï¸ Complete Web Crawler User Guide

## ğŸŒŸ **What Your Crawler Can Do**

Your web crawler is now a **professional-grade tool** with these features:
- âœ… **Smart crawling** with depth control and domain filtering
- âœ… **Polite behavior** respects robots.txt and delays
- âœ… **Configuration files** for easy settings management
- âœ… **Real-time progress** with visual progress bars
- âœ… **Comprehensive statistics** and performance metrics
- âœ… **Intelligent filtering** skips unwanted file types
- âœ… **Detailed logging** with categorized error messages
- âœ… **Organized storage** saves files by domain

---

## ğŸš€ **3 Ways to Use Your Crawler**

### **Method 1: Configuration File (Easiest)**
```bash
# 1. Edit crawler_config.yaml with your settings
# 2. Simply run:
python crawler.py https://example.com
```

### **Method 2: Command Line (Quick)**
```bash
python crawler.py https://example.com --max-pages 20 --delay 2.0
```

### **Method 3: Mixed Approach (Flexible)**
```bash
# Uses config file but overrides specific settings
python crawler.py https://example.com --max-pages 50
```

---

## âš™ï¸ **Configuration File Explained**

Your `crawler_config.yaml` file contains all settings:

```yaml
crawler:
  max_depth: 2                    # How many clicks deep to go
  delay: 1.0                     # Seconds between requests
  max_pages: 50                  # Maximum pages to download
  user_agent: "WebCrawler-Bot/1.0"

output:
  directory: "downloaded_pages"   # Where to save files

filters:
  allowed_domains: []            # Restrict to specific domains
  skip_patterns:                 # Skip URLs containing these
    - "/admin"
    - "/login"
```

### **Key Settings Explained:**

**`max_depth`**: 
- `0` = Only the starting page
- `1` = Start page + all linked pages
- `2` = Start page + links + links from those pages
- `3+` = Goes deeper (be careful!)

**`delay`**:
- `1.0` = 1 second between requests (polite)
- `2.0` = 2 seconds (very polite)
- `0.5` = 0.5 seconds (faster but less polite)

**`max_pages`**:
- `10` = Small test crawl
- `50` = Medium crawl
- `200+` = Large crawl

---

## ğŸ“‹ **All Command Line Options**

```bash
python crawler.py <URL> [OPTIONS]

Required:
  URL                    Starting website to crawl

Optional:
  --config FILE          Configuration file (default: crawler_config.yaml)
  --max-depth N          How deep to crawl (overrides config)
  --delay SECONDS        Delay between requests (overrides config)
  --max-pages N          Maximum pages to download (overrides config)
  --output-dir DIR       Where to save files (overrides config)
  --allowed-domains D1 D2  Only crawl these domains (overrides config)
  --user-agent STRING    User agent string (overrides config)
```

---

## ğŸ¯ **Common Use Cases & Examples**

### **1. Quick Website Backup**
```bash
python crawler.py https://example.com --max-pages 100 --max-depth 2
```

### **2. Test Small Website**
```bash
python crawler.py https://httpbin.org --max-pages 5 --delay 0.5
```

### **3. Deep Documentation Crawl**
```bash
python crawler.py https://docs.example.com --max-depth 3 --max-pages 200
```

### **4. Stay Within One Domain**
```bash
python crawler.py https://example.com --allowed-domains example.com
```

### **5. Slow, Respectful Crawl**
```bash
python crawler.py https://example.com --delay 5.0 --max-pages 20
```

### **6. Custom Output Location**
```bash
python crawler.py https://example.com --output-dir "my-website-backup"
```

---

## ğŸ“Š **Understanding the Output**

### **What You'll See While Crawling:**

```
ğŸ“ Loaded configuration from crawler_config.yaml
ğŸš€ Web Crawler Starting...
ğŸ“‹ Settings: depth=2, pages=50, delay=1.0s
2025-09-21 19:23:20,303 - INFO - Starting crawl from: https://example.com
ğŸ“¥ Downloading:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 17/50 [00:45<01:30, 2.7s/pages, Domain=example.com, Queue=23, Speed=2.1s]
```

**Progress Bar Shows:**
- **33%**: Percentage complete
- **17/50**: Pages downloaded so far
- **Domain**: Current domain being crawled
- **Queue**: How many URLs are waiting
- **Speed**: Average time per page

### **Final Statistics Report:**

```
ğŸ“Š CRAWLING STATISTICS
==================================================
â±ï¸  Duration: 145.2 seconds (2.4 minutes)
ğŸ” URLs Found: 234
âœ… Pages Downloaded: 50
â­ï¸  Pages Skipped: 12
ğŸŒ Domains Crawled: 3
ğŸ’¾ Data Downloaded: 15.67 MB
âš¡ Average Response Time: 2.31s
ğŸ“ˆ Pages/minute: 20.7
âŒ Total Errors: 5
   â””â”€â”€ timeout: 2
   â””â”€â”€ http_errors: 3
ğŸ“Š Success Rate: 89.3%
```

---

## ğŸ“ **File Organization**

Your crawler organizes downloaded files like this:

```
downloaded_pages/           # Main folder
â”œâ”€â”€ crawler.log            # Detailed activity log
â”œâ”€â”€ example.com/           # Domain-based folders
â”‚   â”œâ”€â”€ index.html        # Main page
â”‚   â”œâ”€â”€ index.meta        # Page metadata
â”‚   â”œâ”€â”€ about.html        # About page
â”‚   â”œâ”€â”€ about.meta        # About page metadata
â”‚   â””â”€â”€ products.html     # Products page
â”œâ”€â”€ subdomain.example.com/
â”‚   â”œâ”€â”€ api.html
â”‚   â””â”€â”€ api.meta
â””â”€â”€ cdn.example.com/
    â”œâ”€â”€ docs.html
    â””â”€â”€ docs.meta
```

### **File Types Created:**

**`.html` files**: The actual webpage content
**`.meta` files**: Information about each page:
```
URL: https://example.com/about
Status Code: 200
Content-Type: text/html; charset=utf-8
Content-Length: 15423
Downloaded: 2025-09-21 19:23:45
```

**`crawler.log`**: Complete activity log with timestamps

---

## ğŸ”§ **Advanced Usage Tips**

### **1. Testing New Websites**
Always start small when testing a new website:
```bash
python crawler.py https://new-site.com --max-pages 3 --max-depth 1
```

### **2. Large Crawls**
For big websites, use generous delays:
```bash
python crawler.py https://large-site.com --delay 3.0 --max-pages 500
```

### **3. Domain Restrictions**
Stay within specific domains to avoid wandering:
```yaml
filters:
  allowed_domains: 
    - "example.com"
    - "docs.example.com"
    - "blog.example.com"
```

### **4. Skip Unwanted Content**
Avoid admin areas and search pages:
```yaml
filters:
  skip_patterns:
    - "/admin"
    - "/login"
    - "?search="
    - "/api/"
```

---

## ğŸš¨ **Error Messages & Solutions**

### **Common Errors:**

**âŒ "Timeout error (>30s)"**
â†’ Website is slow. Try: `--delay 2.0`

**âŒ "Rate limited (429)"** 
â†’ Too fast. Try: `--delay 5.0`

**âŒ "Connection error"**
â†’ Check internet or try different website

**âš ï¸ "Robots.txt disallows"**
â†’ Website blocks crawlers for that page (normal)

**âŒ "Access forbidden (403)"**
â†’ Page requires login or is restricted

---

## ğŸ“ˆ **Performance Tips**

### **Speed vs Politeness:**
- **Fast**: `--delay 0.5` (use only for testing)
- **Normal**: `--delay 1.0` (default, good balance)  
- **Polite**: `--delay 3.0` (respectful, recommended)
- **Very polite**: `--delay 5.0` (for sensitive sites)

### **Memory Usage:**
- **Small crawls**: `--max-pages 50` uses minimal memory
- **Large crawls**: `--max-pages 500+` may use more memory

---

## âœ… **Best Practices**

### **1. Always Be Respectful**
- Use delays of 1+ seconds
- Check robots.txt compliance (automatic)
- Don't overload servers

### **2. Start Small**
- Test with `--max-pages 5` first
- Gradually increase for larger crawls
- Monitor the output directory size

### **3. Use Configuration Files**
- Create different configs for different projects
- `crawler_config_news.yaml`, `crawler_config_docs.yaml`

### **4. Monitor Progress**
- Watch the progress bar and statistics
- Check `crawler.log` for detailed information
- Stop if you see many errors (Ctrl+C)

---

## ğŸ¯ **Quick Start Checklist**

1. **âœ… Edit `crawler_config.yaml`** with your preferred settings
2. **âœ… Choose your starting URL**
3. **âœ… Run:** `python crawler.py https://your-target-site.com`
4. **âœ… Watch the progress bar** and statistics
5. **âœ… Check results** in the output directory
6. **âœ… Review `crawler.log`** for detailed information

Your web crawler is now a powerful, professional tool that can handle websites of all sizes while being respectful and providing detailed feedback! ğŸš€